version: "3.8"

x-inference-build: &inference-build
  context: ./src
  dockerfile: inference/Dockerfile.cpu

services:
  preprocessing:
    image: rdsea/preprocessing
    build:
      context: ./src
      dockerfile: preprocessing/Dockerfile
    ports:
      - 5010:5010
    environment:
      DOCKER: "true"

  ensemble:
    image: rdsea/ensemble
    build:
      context: ./src
      dockerfile: ensemble/Dockerfile
    environment:
      DOCKER: "true"
      SEND_TO_QUEUE: true

  efficientnetb0:
    image: rdsea/inference:cpu
    build: *inference-build
    command: ["--model", "EfficientNetB0"]
    # ports:
    #   - "5012:5012"

  mobilenetv2:
    image: rdsea/inference:cpu
    build: *inference-build
    command: ["--model", "MobileNetV2"]
    # ports:
    #   - "5013:5012"
